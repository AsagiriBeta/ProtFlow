{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Prokka â†’ ESM3 â†’ DALI Workflow\n",
    "\n",
    "è¿™ä¸ªå·¥ä½œæµç¨‹å°†ï¼š\n",
    "1. æ¥å— FNAï¼ˆæ ¸é…¸åºåˆ—ï¼‰æ–‡ä»¶ä½œä¸ºè¾“å…¥\n",
    "2. ä½¿ç”¨ Prokka è¿›è¡ŒåŸºå› æ³¨é‡Šå’Œè›‹ç™½è´¨é¢„æµ‹\n",
    "3. å°† Prokka è¾“å‡ºçš„è›‹ç™½è´¨åºåˆ—é€æ¡è¾“å…¥ ESM3 è¿›è¡Œç»“æ„é¢„æµ‹\n",
    "4. ç”Ÿæˆç¬¦åˆ DALI è¾“å…¥æ ‡å‡†çš„ PDB æ–‡ä»¶\n",
    "5. æ‰“åŒ…æ‰€æœ‰ç»“æœä¾›ä¸‹è½½\n",
    "\n",
    "## ç³»ç»Ÿè¦æ±‚\n",
    "- Google Colab (æ¨èä½¿ç”¨ GPU è¿è¡Œæ—¶)\n",
    "- çº¦ 10-20 GB ç£ç›˜ç©ºé—´\n",
    "- è¿è¡Œæ—¶é—´å–å†³äºåºåˆ—æ•°é‡å’Œé•¿åº¦"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. ç¯å¢ƒæ£€æµ‹ä¸è®¾ç½®"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "import os\n",
    "from pathlib import Path\n",
    "\n",
    "# æ£€æµ‹è¿è¡Œç¯å¢ƒ\n",
    "IN_COLAB = 'google.colab' in sys.modules\n",
    "\n",
    "if IN_COLAB:\n",
    "    print(\"âœ“ è¿è¡Œåœ¨ Google Colab\")\n",
    "    from google.colab import files, drive\n",
    "    \n",
    "    # æ£€æŸ¥ GPU\n",
    "    import torch\n",
    "    if torch.cuda.is_available():\n",
    "        print(f\"âœ“ GPU å¯ç”¨: {torch.cuda.get_device_name(0)}\")\n",
    "        print(f\"  æ˜¾å­˜: {torch.cuda.get_device_properties(0).total_memory / 1024**3:.1f} GB\")\n",
    "    else:\n",
    "        print(\"âš  æœªæ£€æµ‹åˆ° GPUï¼Œå»ºè®®åœ¨è¿è¡Œæ—¶è®¾ç½®ä¸­å¯ç”¨ GPU\")\n",
    "else:\n",
    "    print(\"âœ“ è¿è¡Œåœ¨æœ¬åœ°ç¯å¢ƒ\")\n",
    "\n",
    "# è®¾ç½®å·¥ä½œç›®å½•\n",
    "WORK_DIR = Path(\"/content/prokka_esm3_workflow\") if IN_COLAB else Path(\"./prokka_esm3_output\")\n",
    "WORK_DIR.mkdir(exist_ok=True, parents=True)\n",
    "os.chdir(WORK_DIR)\n",
    "\n",
    "print(f\"\\nå·¥ä½œç›®å½•: {WORK_DIR}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. å®‰è£…ä¾èµ–\n",
    "\n",
    "ä½¿ç”¨ mamba åŠ é€Ÿå®‰è£…è¿‡ç¨‹"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# å®‰è£… micromamba å’Œ Prokka\n",
    "import os\n",
    "import subprocess\n",
    "import shutil\n",
    "\n",
    "# æ£€æŸ¥æ˜¯å¦å·²å®‰è£… micromamba\n",
    "if shutil.which('micromamba'):\n",
    "    print('âœ… Micromamba å·²å®‰è£…')\n",
    "else:\n",
    "    print('ğŸ“¥ æ­£åœ¨å®‰è£… micromamba...')\n",
    "    if IN_COLAB:\n",
    "        # åœ¨ Colab ä¸Šå®‰è£… micromamba\n",
    "        !mkdir -p /usr/local/bin\n",
    "        !curl -Ls https://micro.mamba.pm/api/micromamba/linux-64/latest | tar -xj -C /usr/local bin/micromamba\n",
    "        print('âœ… Micromamba å®‰è£…å®Œæˆ')\n",
    "    else:\n",
    "        print('âš ï¸ è¯·æ‰‹åŠ¨å®‰è£… micromamba:')\n",
    "        print('   macOS: brew install micromamba')\n",
    "        print('   Linux: curl -Ls https://micro.mamba.pm/install.sh | bash')\n",
    "\n",
    "# æ£€æŸ¥ prokka ç¯å¢ƒæ˜¯å¦å­˜åœ¨\n",
    "env_check = subprocess.run(\n",
    "    ['micromamba', 'env', 'list'],\n",
    "    capture_output=True,\n",
    "    text=True\n",
    ")\n",
    "\n",
    "if 'prokka' in env_check.stdout:\n",
    "    print('âœ… Prokka ç¯å¢ƒå·²å­˜åœ¨')\n",
    "else:\n",
    "    print('ğŸ“¦ æ­£åœ¨åˆ›å»º Prokka ç¯å¢ƒ...')\n",
    "    print('   (è¿™å¯èƒ½éœ€è¦ 5-10 åˆ†é’Ÿ)')\n",
    "    !micromamba create -y -n prokka -c conda-forge -c bioconda -c defaults prokka python=3.10\n",
    "    print('âœ… Prokka ç¯å¢ƒåˆ›å»ºå®Œæˆ')\n",
    "    \n",
    "    # è®¾ç½® Prokka æ•°æ®åº“\n",
    "    print('ğŸ“¥ æ­£åœ¨è®¾ç½® Prokka æ•°æ®åº“...')\n",
    "    !micromamba run -n prokka prokka --setupdb\n",
    "    print('âœ… Prokka æ•°æ®åº“è®¾ç½®å®Œæˆ')\n",
    "\n",
    "# éªŒè¯å®‰è£…\n",
    "result = subprocess.run(\n",
    "    ['micromamba', 'run', '-n', 'prokka', 'prokka', '--version'],\n",
    "    capture_output=True,\n",
    "    text=True\n",
    ")\n",
    "\n",
    "if result.returncode == 0:\n",
    "    print(f'\\nâœ… Prokka ç‰ˆæœ¬: {result.stdout.strip()}')\n",
    "else:\n",
    "    print('âš ï¸ Prokka éªŒè¯å¤±è´¥')\n",
    "    print(f'é”™è¯¯: {result.stderr[:200]}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Prokka å·²åœ¨ä¸Šä¸€æ­¥å®‰è£…ï¼Œæ­¤å•å…ƒæ ¼å·²åˆå¹¶\n",
    "pass"
   ]
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "# å®‰è£… Python ä¾èµ–\n",
    "!pip install -q esm biopython tqdm torch huggingface_hub\n"
   ]
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "## 3. HuggingFace è®¤è¯\n",
    "\n",
    "**é‡è¦**ï¼šESM3 æ¨¡å‹éœ€è¦ HuggingFace è®¤è¯\n",
    "\n",
    "### å‡†å¤‡å·¥ä½œï¼ˆé¦–æ¬¡ä½¿ç”¨ï¼‰:\n",
    "\n",
    "1. è®¿é—® [ESM3 æ¨¡å‹é¡µé¢](https://huggingface.co/EvolutionaryScale/esm3-sm-open-v1)ï¼Œç‚¹å‡» **\"Request access\"** å¹¶åŒæ„æ¡æ¬¾ï¼ˆè‡ªåŠ¨æ‰¹å‡†ï¼‰\n",
    "2. è®¿é—® [HuggingFace Tokens](https://huggingface.co/settings/tokens)ï¼Œåˆ›å»ºä¸€ä¸ªå…·æœ‰ **Read** æƒé™çš„ token\n",
    "3. åœ¨ä¸‹é¢çš„å•å…ƒæ ¼ä¸­è¾“å…¥ token\n",
    "\n",
    "**å®‰å…¨æç¤º**ï¼šToken æ˜¯ç§å¯†çš„ï¼ŒColab ä¼šè¯ç»“æŸåè‡ªåŠ¨æ¸…é™¤"
   ]
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "# HuggingFace è®¤è¯ï¼ˆå‚ç…§ ProtFlow.ipynbï¼‰\n",
    "from huggingface_hub import login\n",
    "import os\n",
    "\n",
    "print('=' * 60)\n",
    "print('ğŸ”‘ Hugging Face Authentication Required')\n",
    "print('=' * 60)\n",
    "print('To use ESM3 model, you need a Hugging Face token.')\n",
    "print('1. Go to: https://huggingface.co/settings/tokens')\n",
    "print('2. Create a token with READ access')\n",
    "print('3. Grant access to: EvolutionaryScale/esm3-sm-open-v1')\n",
    "print('4. Paste the token below (or set HF_TOKEN env variable)')\n",
    "print('=' * 60)\n",
    "\n",
    "HF_TOKEN = os.getenv('HF_TOKEN')\n",
    "try:\n",
    "    if HF_TOKEN:\n",
    "        print('Using token from HF_TOKEN environment variable...')\n",
    "        login(token=HF_TOKEN)\n",
    "        print('âœ… Logged in successfully!')\n",
    "    else:\n",
    "        login()  # interactive prompt on Colab / local\n",
    "        print('âœ… Logged in successfully!')\n",
    "except Exception as e:\n",
    "    print(f'âŒ Login failed: {e}')\n",
    "    print('Please check your token and try again.')\n",
    "    raise"
   ]
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "## 4. å¯¼å…¥å¿…è¦çš„åº“"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import subprocess\n",
    "import shutil\n",
    "from datetime import datetime\n",
    "from typing import List, Optional\n",
    "import zipfile\n",
    "\n",
    "import torch\n",
    "from Bio import SeqIO\n",
    "from tqdm.auto import tqdm\n",
    "\n",
    "print(\"âœ“ æ‰€æœ‰åº“å¯¼å…¥æˆåŠŸ\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "## 5. å®šä¹‰å·¥ä½œæµå‡½æ•°"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ProkkaESM3Pipeline:\n",
    "    \"\"\"\n",
    "    Prokka -> ESM3 -> DALI å·¥ä½œæµç®¡é“\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, work_dir: Path):\n",
    "        self.work_dir = Path(work_dir)\n",
    "        self.prokka_dir = self.work_dir / \"prokka_output\"\n",
    "        self.pdb_dir = self.work_dir / \"esm3_structures\"\n",
    "        self.dali_dir = self.work_dir / \"dali_ready\"\n",
    "        \n",
    "        # åˆ›å»ºç›®å½•\n",
    "        for d in [self.prokka_dir, self.pdb_dir, self.dali_dir]:\n",
    "            d.mkdir(exist_ok=True, parents=True)\n",
    "        \n",
    "        self.model = None\n",
    "        self.device = None\n",
    "    \n",
    "    def run_prokka(self, \n",
    "                   fna_file: Path, \n",
    "                   prefix: str = \"sample\",\n",
    "                   kingdom: str = \"Bacteria\",\n",
    "                   cpus: int = 2,\n",
    "                   **kwargs) -> Path:\n",
    "        \"\"\"\n",
    "        è¿è¡Œ Prokka è¿›è¡ŒåŸºå› æ³¨é‡Š\n",
    "        \n",
    "        Args:\n",
    "            fna_file: è¾“å…¥çš„ FNA æ–‡ä»¶è·¯å¾„\n",
    "            prefix: è¾“å‡ºæ–‡ä»¶å‰ç¼€\n",
    "            kingdom: ç”Ÿç‰©ç•Œï¼ˆBacteria, Archaea, Virusesï¼‰\n",
    "            cpus: ä½¿ç”¨çš„ CPU æ ¸å¿ƒæ•°\n",
    "            **kwargs: å…¶ä»– Prokka å‚æ•°\n",
    "        \n",
    "        Returns:\n",
    "            Prokka è¾“å‡ºç›®å½•è·¯å¾„\n",
    "        \"\"\"\n",
    "        print(f\"\\n{'='*60}\")\n",
    "        print(\"æ­¥éª¤ 1: è¿è¡Œ Prokka è¿›è¡ŒåŸºå› æ³¨é‡Š\")\n",
    "        print(f\"{'='*60}\")\n",
    "        \n",
    "        output_dir = self.prokka_dir / prefix\n",
    "        \n",
    "        # æ„å»º Prokka å‘½ä»¤ - ä½¿ç”¨ micromamba run\n",
    "        cmd = [\n",
    "            \"micromamba\", \"run\", \"-n\", \"prokka\", \"prokka\",\n",
    "            \"--outdir\", str(output_dir),\n",
    "            \"--prefix\", prefix,\n",
    "            \"--kingdom\", kingdom,\n",
    "            \"--cpus\", str(cpus),\n",
    "            \"--force\",  # è¦†ç›–å·²å­˜åœ¨çš„è¾“å‡º\n",
    "        ]\n",
    "        \n",
    "        # æ·»åŠ é¢å¤–å‚æ•°\n",
    "        for key, value in kwargs.items():\n",
    "            cmd.extend([f\"--{key}\", str(value)])\n",
    "        \n",
    "        cmd.append(str(fna_file))\n",
    "        \n",
    "        print(f\"è¿è¡Œå‘½ä»¤: {' '.join(cmd)}\")\n",
    "        print(\"\\næ­£åœ¨è¿è¡Œ Prokkaï¼ˆè¿™å¯èƒ½éœ€è¦å‡ åˆ†é’Ÿï¼‰...\")\n",
    "        \n",
    "        try:\n",
    "            result = subprocess.run(cmd, check=True, capture_output=True, text=True)\n",
    "            print(\"\\nâœ“ Prokka è¿è¡ŒæˆåŠŸï¼\")\n",
    "            \n",
    "            # æ˜¾ç¤ºç»Ÿè®¡ä¿¡æ¯\n",
    "            stats_file = output_dir / f\"{prefix}.txt\"\n",
    "            if stats_file.exists():\n",
    "                print(\"\\næ³¨é‡Šç»Ÿè®¡:\")\n",
    "                print(stats_file.read_text())\n",
    "            \n",
    "            return output_dir\n",
    "            \n",
    "        except subprocess.CalledProcessError as e:\n",
    "            print(f\"\\nâœ— Prokka è¿è¡Œå¤±è´¥: {e}\")\n",
    "            print(f\"é”™è¯¯è¾“å‡º: {e.stderr}\")\n",
    "            raise\n",
    "    \n",
    "    def load_esm3_model(self, model_name: str = 'esm3-sm-open-v1'):\n",
    "        \"\"\"\n",
    "        åŠ è½½ ESM3 æ¨¡å‹ï¼ˆå‚ç…§ ProtFlow.ipynb çš„å®ç°ï¼‰\n",
    "\n",
    "        Args:\n",
    "            model_name: ESM3 æ¨¡å‹åç§°\n",
    "        \"\"\"\n",
    "        print(f\"\\n{'='*60}\")\n",
    "        print(\"æ­¥éª¤ 2: åŠ è½½ ESM3 æ¨¡å‹\")\n",
    "        print(f\"{'='*60}\")\n",
    "        \n",
    "        # è‡ªåŠ¨æ£€æµ‹è®¾å¤‡\n",
    "        if torch.cuda.is_available():\n",
    "            self.device = 'cuda'\n",
    "            print(f\"âœ… GPU detected: {torch.cuda.get_device_name(0)}\")\n",
    "            print(f\"   Memory: {torch.cuda.get_device_properties(0).total_memory / 1e9:.1f} GB\")\n",
    "        else:\n",
    "            self.device = 'cpu'\n",
    "            print(\"âš ï¸ No GPU detected. Model will run on CPU (slower).\")\n",
    "            print(\"   Tip: In Colab, enable GPU via Runtime â†’ Change runtime type â†’ GPU\")\n",
    "\n",
    "        print()\n",
    "        print(f\"Loading ESM3-sm model (this may take a few minutes)...\")\n",
    "\n",
    "        try:\n",
    "            from esm.models.esm3 import ESM3\n",
    "\n",
    "            # from_pretrained ä¼šè‡ªåŠ¨ä½¿ç”¨å·²ç™»å½•çš„ HuggingFace session\n",
    "            self.model = ESM3.from_pretrained(model_name).to(self.device)\n",
    "            self.model.eval()\n",
    "\n",
    "            print(f\"âœ… Model loaded successfully on {self.device}\")\n",
    "            print()\n",
    "        except Exception as e:\n",
    "            print(f\"âŒ Failed to load model: {e}\")\n",
    "            print(\"\\nCommon issues:\")\n",
    "            print(\"  - Insufficient GPU memory (try restarting runtime)\")\n",
    "            print(\"  - Network timeout (try running the cell again)\")\n",
    "            print(\"  - Missing HuggingFace token (re-run Cell 3)\")\n",
    "            print(\"  - Missing ESM3 access (visit https://huggingface.co/EvolutionaryScale/esm3-sm-open-v1)\")\n",
    "            raise\n",
    "\n",
    "    def predict_structures(self,\n",
    "                          prokka_dir: Path,\n",
    "                          prefix: str,\n",
    "                          num_steps: int = 8,\n",
    "                          max_length: int = 400,\n",
    "                          min_length: int = 30) -> List[Path]:\n",
    "        \"\"\"\n",
    "        ä½¿ç”¨ ESM3 é¢„æµ‹è›‹ç™½è´¨ç»“æ„\n",
    "        \n",
    "        Args:\n",
    "            prokka_dir: Prokka è¾“å‡ºç›®å½•\n",
    "            prefix: Prokka è¾“å‡ºå‰ç¼€\n",
    "            num_steps: ESM3 ç”Ÿæˆæ­¥æ•°\n",
    "            max_length: æœ€å¤§åºåˆ—é•¿åº¦ï¼ˆè¿‡é•¿çš„åºåˆ—ä¼šè¢«è·³è¿‡ï¼‰\n",
    "            min_length: æœ€å°åºåˆ—é•¿åº¦\n",
    "        \n",
    "        Returns:\n",
    "            ç”Ÿæˆçš„ PDB æ–‡ä»¶è·¯å¾„åˆ—è¡¨\n",
    "        \"\"\"\n",
    "        print(f\"\\n{'='*60}\")\n",
    "        print(\"æ­¥éª¤ 3: ä½¿ç”¨ ESM3 é¢„æµ‹è›‹ç™½è´¨ç»“æ„\")\n",
    "        print(f\"{'='*60}\")\n",
    "        \n",
    "        if self.model is None:\n",
    "            self.load_esm3_model()\n",
    "        \n",
    "        from esm.sdk.api import ESMProtein, GenerationConfig\n",
    "        \n",
    "        # è¯»å– Prokka è¾“å‡ºçš„è›‹ç™½è´¨åºåˆ—\n",
    "        faa_file = prokka_dir / f\"{prefix}.faa\"\n",
    "        \n",
    "        if not faa_file.exists():\n",
    "            raise FileNotFoundError(f\"æ‰¾ä¸åˆ° Prokka è›‹ç™½è´¨æ–‡ä»¶: {faa_file}\")\n",
    "        \n",
    "        # è§£æåºåˆ—\n",
    "        sequences = list(SeqIO.parse(faa_file, \"fasta\"))\n",
    "        print(f\"\\nä» Prokka è¯»å–åˆ° {len(sequences)} æ¡è›‹ç™½è´¨åºåˆ—\")\n",
    "        \n",
    "        # è¿‡æ»¤åºåˆ—\n",
    "        filtered_seqs = [\n",
    "            seq for seq in sequences \n",
    "            if min_length <= len(seq.seq) <= max_length\n",
    "        ]\n",
    "        \n",
    "        skipped = len(sequences) - len(filtered_seqs)\n",
    "        if skipped > 0:\n",
    "            print(f\"è·³è¿‡ {skipped} æ¡åºåˆ—ï¼ˆé•¿åº¦ä¸åœ¨ {min_length}-{max_length} èŒƒå›´å†…ï¼‰\")\n",
    "        \n",
    "        print(f\"å°†é¢„æµ‹ {len(filtered_seqs)} æ¡åºåˆ—çš„ç»“æ„\\n\")\n",
    "        \n",
    "        pdb_files = []\n",
    "        success_count = 0\n",
    "        error_count = 0\n",
    "        \n",
    "        # é€æ¡é¢„æµ‹\n",
    "        for rec in tqdm(filtered_seqs, desc=\"é¢„æµ‹ç»“æ„\"):\n",
    "            try:\n",
    "                seq = str(rec.seq)\n",
    "                # æ¸…ç† ID ä»¥ç”¨ä½œæ–‡ä»¶å\n",
    "                name = rec.id.replace('|', '_').replace('/', '_').replace('\\\\', '_')[:100]\n",
    "                pdb_file = self.pdb_dir / f\"{name}.pdb\"\n",
    "                \n",
    "                # è·³è¿‡å·²å­˜åœ¨çš„æ–‡ä»¶\n",
    "                if pdb_file.exists():\n",
    "                    pdb_files.append(pdb_file)\n",
    "                    success_count += 1\n",
    "                    continue\n",
    "                \n",
    "                # ä½¿ç”¨ ESM3 ç”Ÿæˆç»“æ„\n",
    "                protein = ESMProtein(sequence=seq)\n",
    "                protein = self.model.generate(\n",
    "                    protein, \n",
    "                    GenerationConfig(track='structure', num_steps=num_steps)\n",
    "                )\n",
    "                \n",
    "                # ä¿å­˜ä¸º PDB\n",
    "                protein.to_pdb(str(pdb_file))\n",
    "                pdb_files.append(pdb_file)\n",
    "                success_count += 1\n",
    "                \n",
    "            except Exception as e:\n",
    "                print(f\"\\né¢„æµ‹å¤±è´¥ {rec.id}: {e}\")\n",
    "                error_count += 1\n",
    "                continue\n",
    "        \n",
    "        print(f\"\\nâœ“ ç»“æ„é¢„æµ‹å®Œæˆï¼\")\n",
    "        print(f\"  æˆåŠŸ: {success_count}\")\n",
    "        print(f\"  å¤±è´¥: {error_count}\")\n",
    "        \n",
    "        return pdb_files\n",
    "    \n",
    "    def prepare_for_dali(self, pdb_files: List[Path]) -> Path:\n",
    "        \"\"\"\n",
    "        å‡†å¤‡ç¬¦åˆ DALI è¾“å…¥æ ‡å‡†çš„æ–‡ä»¶\n",
    "        \n",
    "        Args:\n",
    "            pdb_files: PDB æ–‡ä»¶è·¯å¾„åˆ—è¡¨\n",
    "        \n",
    "        Returns:\n",
    "            DALI è¾“å‡ºç›®å½•è·¯å¾„\n",
    "        \"\"\"\n",
    "        print(f\"\\n{'='*60}\")\n",
    "        print(\"æ­¥éª¤ 4: å‡†å¤‡ DALI è¾“å…¥æ–‡ä»¶\")\n",
    "        print(f\"{'='*60}\")\n",
    "        \n",
    "        # å¤åˆ¶ PDB æ–‡ä»¶åˆ° DALI ç›®å½•\n",
    "        for pdb_file in tqdm(pdb_files, desc=\"å¤åˆ¶æ–‡ä»¶\"):\n",
    "            dest = self.dali_dir / pdb_file.name\n",
    "            if not dest.exists():\n",
    "                shutil.copy2(pdb_file, dest)\n",
    "        \n",
    "        # åˆ›å»ºæ–‡ä»¶åˆ—è¡¨\n",
    "        file_list = self.dali_dir / \"pdb_list.txt\"\n",
    "        with open(file_list, 'w') as f:\n",
    "            for pdb_file in pdb_files:\n",
    "                f.write(f\"{pdb_file.name}\\n\")\n",
    "        \n",
    "        # åˆ›å»º README\n",
    "        readme = self.dali_dir / \"README.txt\"\n",
    "        with open(readme, 'w') as f:\n",
    "            f.write(\"DALI è¾“å…¥æ–‡ä»¶\\n\")\n",
    "            f.write(\"=\"*60 + \"\\n\\n\")\n",
    "            f.write(f\"ç”Ÿæˆæ—¶é—´: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}\\n\")\n",
    "            f.write(f\"PDB æ–‡ä»¶æ•°é‡: {len(pdb_files)}\\n\\n\")\n",
    "            f.write(\"ä½¿ç”¨è¯´æ˜:\\n\")\n",
    "            f.write(\"1. è¿™äº› PDB æ–‡ä»¶å¯ç›´æ¥ç”¨äº DALI ç»“æ„æ¯”å¯¹\\n\")\n",
    "            f.write(\"2. è®¿é—® DALI æœåŠ¡å™¨: http://ekhidna2.biocenter.helsinki.fi/dali/\\n\")\n",
    "            f.write(\"3. ä¸Šä¼ å•ä¸ªæˆ–å¤šä¸ª PDB æ–‡ä»¶è¿›è¡Œæ¯”å¯¹åˆ†æ\\n\")\n",
    "            f.write(\"4. pdb_list.txt åŒ…å«æ‰€æœ‰ PDB æ–‡ä»¶çš„åˆ—è¡¨\\n\")\n",
    "        \n",
    "        print(f\"\\nâœ“ DALI æ–‡ä»¶å‡†å¤‡å®Œæˆï¼\")\n",
    "        print(f\"  ä½ç½®: {self.dali_dir}\")\n",
    "        print(f\"  æ–‡ä»¶æ•°: {len(pdb_files)}\")\n",
    "        \n",
    "        return self.dali_dir\n",
    "    \n",
    "    def create_download_package(self, prefix: str) -> Path:\n",
    "        \"\"\"\n",
    "        åˆ›å»ºå¯ä¸‹è½½çš„å‹ç¼©åŒ…\n",
    "        \n",
    "        Args:\n",
    "            prefix: è¾“å‡ºæ–‡ä»¶å‰ç¼€\n",
    "        \n",
    "        Returns:\n",
    "            å‹ç¼©åŒ…è·¯å¾„\n",
    "        \"\"\"\n",
    "        print(f\"\\n{'='*60}\")\n",
    "        print(\"æ­¥éª¤ 5: åˆ›å»ºä¸‹è½½åŒ…\")\n",
    "        print(f\"{'='*60}\")\n",
    "        \n",
    "        timestamp = datetime.now().strftime('%Y%m%d_%H%M%S')\n",
    "        zip_file = self.work_dir / f\"{prefix}_results_{timestamp}.zip\"\n",
    "        \n",
    "        with zipfile.ZipFile(zip_file, 'w', zipfile.ZIP_DEFLATED) as zf:\n",
    "            # æ·»åŠ  Prokka ç»“æœ\n",
    "            print(\"\\næ‰“åŒ… Prokka ç»“æœ...\")\n",
    "            for file in self.prokka_dir.rglob('*'):\n",
    "                if file.is_file():\n",
    "                    arcname = file.relative_to(self.work_dir)\n",
    "                    zf.write(file, arcname)\n",
    "            \n",
    "            # æ·»åŠ  ESM3 ç»“æ„\n",
    "            print(\"æ‰“åŒ… ESM3 ç»“æ„...\")\n",
    "            for file in self.pdb_dir.rglob('*.pdb'):\n",
    "                arcname = file.relative_to(self.work_dir)\n",
    "                zf.write(file, arcname)\n",
    "            \n",
    "            # æ·»åŠ  DALI æ–‡ä»¶\n",
    "            print(\"æ‰“åŒ… DALI æ–‡ä»¶...\")\n",
    "            for file in self.dali_dir.rglob('*'):\n",
    "                if file.is_file():\n",
    "                    arcname = file.relative_to(self.work_dir)\n",
    "                    zf.write(file, arcname)\n",
    "        \n",
    "        size_mb = zip_file.stat().st_size / (1024 * 1024)\n",
    "        print(f\"\\nâœ“ å‹ç¼©åŒ…åˆ›å»ºå®Œæˆï¼\")\n",
    "        print(f\"  æ–‡ä»¶: {zip_file.name}\")\n",
    "        print(f\"  å¤§å°: {size_mb:.2f} MB\")\n",
    "        \n",
    "        return zip_file\n",
    "    \n",
    "    def run_full_pipeline(self,\n",
    "                         fna_file: Path,\n",
    "                         prefix: str = \"sample\",\n",
    "                         kingdom: str = \"Bacteria\",\n",
    "                         num_steps: int = 8,\n",
    "                         max_seq_length: int = 400,\n",
    "                         **prokka_kwargs) -> Path:\n",
    "        \"\"\"\n",
    "        è¿è¡Œå®Œæ•´å·¥ä½œæµ\n",
    "        \n",
    "        Args:\n",
    "            fna_file: è¾“å…¥çš„ FNA æ–‡ä»¶\n",
    "            prefix: è¾“å‡ºæ–‡ä»¶å‰ç¼€\n",
    "            kingdom: ç”Ÿç‰©ç•Œ\n",
    "            num_steps: ESM3 ç”Ÿæˆæ­¥æ•°\n",
    "            max_seq_length: æœ€å¤§åºåˆ—é•¿åº¦\n",
    "            **prokka_kwargs: Prokka é¢å¤–å‚æ•°\n",
    "        \n",
    "        Returns:\n",
    "            ä¸‹è½½åŒ…è·¯å¾„\n",
    "        \"\"\"\n",
    "        start_time = datetime.now()\n",
    "        print(f\"\\n{'='*60}\")\n",
    "        print(\"Prokka â†’ ESM3 â†’ DALI å·¥ä½œæµ\")\n",
    "        print(f\"{'='*60}\")\n",
    "        print(f\"å¼€å§‹æ—¶é—´: {start_time.strftime('%Y-%m-%d %H:%M:%S')}\")\n",
    "        print(f\"è¾“å…¥æ–‡ä»¶: {fna_file}\")\n",
    "        print(f\"è¾“å‡ºå‰ç¼€: {prefix}\")\n",
    "        \n",
    "        # æ­¥éª¤ 1: Prokka æ³¨é‡Š\n",
    "        prokka_dir = self.run_prokka(fna_file, prefix, kingdom, **prokka_kwargs)\n",
    "        \n",
    "        # æ­¥éª¤ 2-3: ESM3 ç»“æ„é¢„æµ‹\n",
    "        pdb_files = self.predict_structures(\n",
    "            prokka_dir, \n",
    "            prefix, \n",
    "            num_steps=num_steps,\n",
    "            max_length=max_seq_length\n",
    "        )\n",
    "        \n",
    "        # æ­¥éª¤ 4: å‡†å¤‡ DALI æ–‡ä»¶\n",
    "        self.prepare_for_dali(pdb_files)\n",
    "        \n",
    "        # æ­¥éª¤ 5: åˆ›å»ºä¸‹è½½åŒ…\n",
    "        zip_file = self.create_download_package(prefix)\n",
    "        \n",
    "        end_time = datetime.now()\n",
    "        duration = end_time - start_time\n",
    "        \n",
    "        print(f\"\\n{'='*60}\")\n",
    "        print(\"âœ“ å·¥ä½œæµå®Œæˆï¼\")\n",
    "        print(f\"{'='*60}\")\n",
    "        print(f\"æ€»è€—æ—¶: {duration}\")\n",
    "        print(f\"\\nç»“æœæ–‡ä»¶: {zip_file}\")\n",
    "        \n",
    "        return zip_file\n",
    "\n",
    "print(\"âœ“ å·¥ä½œæµç±»å®šä¹‰å®Œæˆ\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. ä¸Šä¼ è¾“å…¥æ–‡ä»¶\n",
    "\n",
    "ä¸Šä¼ ä½ çš„ FNAï¼ˆæ ¸é…¸åºåˆ—ï¼‰æ–‡ä»¶"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if IN_COLAB:\n",
    "    print(\"è¯·ä¸Šä¼  FNA æ–‡ä»¶...\")\n",
    "    uploaded = files.upload()\n",
    "    \n",
    "    # è·å–ä¸Šä¼ çš„æ–‡ä»¶\n",
    "    fna_files = [f for f in uploaded.keys() if f.endswith(('.fna', '.fa', '.fasta'))]\n",
    "    \n",
    "    if not fna_files:\n",
    "        raise ValueError(\"æœªæ‰¾åˆ° FNA æ–‡ä»¶ï¼Œè¯·ç¡®ä¿ä¸Šä¼ çš„æ˜¯ .fna/.fa/.fasta æ ¼å¼çš„æ–‡ä»¶\")\n",
    "    \n",
    "    input_fna = Path(fna_files[0])\n",
    "    print(f\"\\nâœ“ ä¸Šä¼ æˆåŠŸ: {input_fna}\")\n",
    "    print(f\"  æ–‡ä»¶å¤§å°: {input_fna.stat().st_size / 1024:.2f} KB\")\n",
    "else:\n",
    "    # æœ¬åœ°ç¯å¢ƒï¼šæ‰‹åŠ¨æŒ‡å®šæ–‡ä»¶è·¯å¾„\n",
    "    input_fna = Path(\"example.fna\")  # ä¿®æ”¹ä¸ºä½ çš„æ–‡ä»¶è·¯å¾„\n",
    "    \n",
    "    if not input_fna.exists():\n",
    "        print(f\"é”™è¯¯: æ–‡ä»¶ä¸å­˜åœ¨ {input_fna}\")\n",
    "        print(\"è¯·ä¿®æ”¹ä¸Šé¢çš„ä»£ç ï¼ŒæŒ‡å®šæ­£ç¡®çš„ FNA æ–‡ä»¶è·¯å¾„\")\n",
    "    else:\n",
    "        print(f\"âœ“ è¾“å…¥æ–‡ä»¶: {input_fna}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "## 7. é…ç½®å·¥ä½œæµå‚æ•°"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# åŸºæœ¬å‚æ•°\n",
    "OUTPUT_PREFIX = \"my_genome\"  # è¾“å‡ºæ–‡ä»¶å‰ç¼€\n",
    "KINGDOM = \"Bacteria\"  # ç”Ÿç‰©ç•Œ: Bacteria, Archaea, æˆ– Viruses\n",
    "\n",
    "# Prokka å‚æ•°\n",
    "GENUS = None  # ä¾‹å¦‚: \"Escherichia\" (å¯é€‰)\n",
    "SPECIES = None  # ä¾‹å¦‚: \"coli\" (å¯é€‰)\n",
    "STRAIN = None  # ä¾‹å¦‚: \"K12\" (å¯é€‰)\n",
    "\n",
    "# ESM3 å‚æ•°\n",
    "NUM_STEPS = 8  # ç”Ÿæˆæ­¥æ•° (8-16ï¼Œè¶Šå¤§è¶Šæ…¢ä½†è´¨é‡å¯èƒ½æ›´å¥½)\n",
    "MAX_SEQ_LENGTH = 400  # æœ€å¤§åºåˆ—é•¿åº¦ï¼ˆè¶…è¿‡æ­¤é•¿åº¦çš„åºåˆ—ä¼šè¢«è·³è¿‡ï¼‰\n",
    "MIN_SEQ_LENGTH = 30  # æœ€å°åºåˆ—é•¿åº¦\n",
    "\n",
    "# CPU æ ¸å¿ƒæ•°\n",
    "CPUS = 2\n",
    "\n",
    "print(\"é…ç½®å‚æ•°:\")\n",
    "print(f\"  è¾“å‡ºå‰ç¼€: {OUTPUT_PREFIX}\")\n",
    "print(f\"  ç”Ÿç‰©ç•Œ: {KINGDOM}\")\n",
    "print(f\"  ESM3 æ­¥æ•°: {NUM_STEPS}\")\n",
    "print(f\"  åºåˆ—é•¿åº¦èŒƒå›´: {MIN_SEQ_LENGTH}-{MAX_SEQ_LENGTH}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8. è¿è¡Œ Prokka åŸºå› æ³¨é‡Š\n",
    "\n",
    "é¦–å…ˆè¿è¡Œ Prokka è¿›è¡ŒåŸºå› æ³¨é‡Š"
   ]
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "# åˆ›å»ºç®¡é“å®ä¾‹\n",
    "pipeline = ProkkaESM3Pipeline(WORK_DIR)\n",
    "\n",
    "# å‡†å¤‡ Prokka å‚æ•°\n",
    "prokka_params = {\n",
    "    'cpus': CPUS,\n",
    "}\n",
    "\n",
    "if GENUS:\n",
    "    prokka_params['genus'] = GENUS\n",
    "if SPECIES:\n",
    "    prokka_params['species'] = SPECIES\n",
    "if STRAIN:\n",
    "    prokka_params['strain'] = STRAIN\n",
    "\n",
    "# è¿è¡Œ Prokka\n",
    "try:\n",
    "    print(f\"\\n{'='*60}\")\n",
    "    print(\"æ­¥éª¤ 1: Prokka åŸºå› æ³¨é‡Š\")\n",
    "    print(f\"{'='*60}\")\n",
    "\n",
    "    prokka_dir = pipeline.run_prokka(\n",
    "        fna_file=input_fna,\n",
    "        prefix=OUTPUT_PREFIX,\n",
    "        kingdom=KINGDOM,\n",
    "        **prokka_params\n",
    "    )\n",
    "    \n",
    "    # ç»Ÿè®¡ Prokka ç»“æœ\n",
    "    prokka_faa = prokka_dir / f\"{OUTPUT_PREFIX}.faa\"\n",
    "    if prokka_faa.exists():\n",
    "        all_proteins = list(SeqIO.parse(prokka_faa, \"fasta\"))\n",
    "\n",
    "        # è¿‡æ»¤åºåˆ—\n",
    "        filtered_proteins = [\n",
    "            seq for seq in all_proteins\n",
    "            if MIN_SEQ_LENGTH <= len(seq.seq) <= MAX_SEQ_LENGTH\n",
    "        ]\n",
    "\n",
    "        print(f\"\\n{'='*60}\")\n",
    "        print(\"Prokka æ³¨é‡Šå®Œæˆï¼\")\n",
    "        print(f\"{'='*60}\")\n",
    "        print(f\"æ€»è›‹ç™½è´¨æ•°: {len(all_proteins)}\")\n",
    "        print(f\"ç¬¦åˆé•¿åº¦è¦æ±‚çš„è›‹ç™½è´¨: {len(filtered_proteins)} (é•¿åº¦: {MIN_SEQ_LENGTH}-{MAX_SEQ_LENGTH})\")\n",
    "        print(f\"ä¸ç¬¦åˆè¦æ±‚çš„: {len(all_proteins) - len(filtered_proteins)}\")\n",
    "\n",
    "        # æ˜¾ç¤ºé•¿åº¦åˆ†å¸ƒ\n",
    "        lengths = [len(seq.seq) for seq in all_proteins]\n",
    "        print(f\"\\nåºåˆ—é•¿åº¦ç»Ÿè®¡:\")\n",
    "        print(f\"  æœ€çŸ­: {min(lengths)} aa\")\n",
    "        print(f\"  æœ€é•¿: {max(lengths)} aa\")\n",
    "        print(f\"  å¹³å‡: {sum(lengths)/len(lengths):.1f} aa\")\n",
    "\n",
    "    print(f\"\\nâœ“ Prokka ç»“æœä¿å­˜åœ¨: {prokka_dir}\")\n",
    "\n",
    "except Exception as e:\n",
    "    print(f\"\\n{'='*60}\")\n",
    "    print(\"Prokka è¿è¡Œå¤±è´¥\")\n",
    "    print(f\"{'='*60}\")\n",
    "    print(f\"é”™è¯¯: {e}\")\n",
    "    import traceback\n",
    "    traceback.print_exc()\n",
    "    raise"
   ]
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "## 9. é€‰æ‹©è¦é¢„æµ‹ç»“æ„çš„åºåˆ—\n",
    "\n",
    "âš ï¸ **é‡è¦**ï¼šå¦‚æœ Prokka å‘ç°çš„è›‹ç™½è´¨æ•°é‡å¾ˆå¤šï¼ŒESM3 é¢„æµ‹å¯èƒ½éœ€è¦å¾ˆé•¿æ—¶é—´ã€‚\n",
    "\n",
    "ä½ å¯ä»¥é€‰æ‹©ï¼š\n",
    "- é¢„æµ‹å…¨éƒ¨åºåˆ—\n",
    "- åªé¢„æµ‹å‰ N ä¸ªåºåˆ—\n",
    "- æŒ‰é•¿åº¦æ’åºåé€‰æ‹©\n",
    "\n",
    "**å»ºè®®**ï¼š\n",
    "- Colab å…è´¹ç‰ˆæœ‰æ—¶é—´é™åˆ¶ï¼Œå»ºè®®ä¸€æ¬¡é¢„æµ‹ä¸è¶…è¿‡ 50-100 ä¸ªåºåˆ—\n",
    "- å¦‚æœåºåˆ—å¾ˆå¤šï¼Œå¯ä»¥åˆ†æ‰¹è¿è¡Œ"
   ]
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "# é…ç½®è¦é¢„æµ‹çš„åºåˆ—æ•°é‡\n",
    "PREDICT_MODE = \"first_n\"  # é€‰é¡¹: \"all\" (å…¨éƒ¨), \"first_n\" (å‰Nä¸ª), \"random\" (éšæœºNä¸ª)\n",
    "PREDICT_COUNT = 50  # å¦‚æœé€‰æ‹© \"first_n\" æˆ– \"random\"ï¼ŒæŒ‡å®šæ•°é‡\n",
    "\n",
    "# è¯»å– Prokka ç»“æœ\n",
    "prokka_faa = prokka_dir / f\"{OUTPUT_PREFIX}.faa\"\n",
    "all_proteins = list(SeqIO.parse(prokka_faa, \"fasta\"))\n",
    "\n",
    "# è¿‡æ»¤é•¿åº¦\n",
    "filtered_proteins = [\n",
    "    seq for seq in all_proteins\n",
    "    if MIN_SEQ_LENGTH <= len(seq.seq) <= MAX_SEQ_LENGTH\n",
    "]\n",
    "\n",
    "# é€‰æ‹©è¦é¢„æµ‹çš„åºåˆ—\n",
    "if PREDICT_MODE == \"all\":\n",
    "    selected_proteins = filtered_proteins\n",
    "    print(f\"é€‰æ‹©æ¨¡å¼: é¢„æµ‹å…¨éƒ¨åºåˆ—\")\n",
    "elif PREDICT_MODE == \"first_n\":\n",
    "    selected_proteins = filtered_proteins[:PREDICT_COUNT]\n",
    "    print(f\"é€‰æ‹©æ¨¡å¼: é¢„æµ‹å‰ {PREDICT_COUNT} ä¸ªåºåˆ—\")\n",
    "elif PREDICT_MODE == \"random\":\n",
    "    import random\n",
    "    selected_proteins = random.sample(filtered_proteins, min(PREDICT_COUNT, len(filtered_proteins)))\n",
    "    print(f\"é€‰æ‹©æ¨¡å¼: éšæœºé€‰æ‹© {len(selected_proteins)} ä¸ªåºåˆ—\")\n",
    "else:\n",
    "    selected_proteins = filtered_proteins\n",
    "    print(f\"æœªçŸ¥æ¨¡å¼ï¼Œä½¿ç”¨å…¨éƒ¨åºåˆ—\")\n",
    "\n",
    "print(f\"\\n{'='*60}\")\n",
    "print(\"åºåˆ—é€‰æ‹©ç»“æœ\")\n",
    "print(f\"{'='*60}\")\n",
    "print(f\"æ€»åºåˆ—æ•°: {len(all_proteins)}\")\n",
    "print(f\"ç¬¦åˆé•¿åº¦è¦æ±‚: {len(filtered_proteins)}\")\n",
    "print(f\"å°†è¦é¢„æµ‹: {len(selected_proteins)}\")\n",
    "\n",
    "if len(selected_proteins) > 0:\n",
    "    # ä¼°ç®—æ—¶é—´\n",
    "    avg_time_per_seq = 30  # ç§’ï¼ˆç²—ç•¥ä¼°è®¡ï¼‰\n",
    "    estimated_minutes = (len(selected_proteins) * avg_time_per_seq) / 60\n",
    "    print(f\"\\né¢„è®¡è€—æ—¶: çº¦ {estimated_minutes:.1f} åˆ†é’Ÿ\")\n",
    "\n",
    "    if estimated_minutes > 60:\n",
    "        print(f\"âš ï¸ é¢„è®¡æ—¶é—´è¾ƒé•¿ï¼Œå»ºè®®å‡å°‘åºåˆ—æ•°é‡æˆ–åˆ†æ‰¹è¿è¡Œ\")\n",
    "else:\n",
    "    print(\"\\nâš ï¸ æ²¡æœ‰ç¬¦åˆæ¡ä»¶çš„åºåˆ—ï¼Œè¯·è°ƒæ•´å‚æ•°\")\n",
    "\n",
    "# æ˜¾ç¤ºå‰å‡ ä¸ªå°†è¦é¢„æµ‹çš„åºåˆ—\n",
    "if len(selected_proteins) > 0:\n",
    "    print(f\"\\nå‰ 5 ä¸ªå°†è¦é¢„æµ‹çš„åºåˆ—:\")\n",
    "    for i, seq in enumerate(selected_proteins[:5], 1):\n",
    "        print(f\"  {i}. {seq.id[:50]} (é•¿åº¦: {len(seq.seq)} aa)\")"
   ]
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "## 10. è¿è¡Œ ESM3 ç»“æ„é¢„æµ‹\n",
    "\n",
    "âš ï¸ **æ³¨æ„**ï¼šè¿™ä¸€æ­¥å¯èƒ½éœ€è¦è¾ƒé•¿æ—¶é—´ï¼Œè¯·ç¡®ä¿ï¼š\n",
    "1. å·²å¯ç”¨ GPU\n",
    "2. æœ‰è¶³å¤Ÿçš„æ—¶é—´ï¼ˆå»ºè®®åºåˆ—æ•° < 100ï¼‰\n",
    "3. å¯ä»¥éšæ—¶åœæ­¢å¹¶ä¿å­˜å·²å®Œæˆçš„ç»“æœ"
   ]
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "# è¿è¡Œ ESM3 ç»“æ„é¢„æµ‹\n",
    "try:\n",
    "    print(f\"\\n{'='*60}\")\n",
    "    print(\"æ­¥éª¤ 2: ESM3 ç»“æ„é¢„æµ‹\")\n",
    "    print(f\"{'='*60}\")\n",
    "    print(f\"å°†é¢„æµ‹ {len(selected_proteins)} ä¸ªè›‹ç™½è´¨ç»“æ„\\n\")\n",
    "\n",
    "    # åŠ è½½ ESM3 æ¨¡å‹\n",
    "    if pipeline.model is None:\n",
    "        pipeline.load_esm3_model()\n",
    "\n",
    "    # å¼€å§‹é¢„æµ‹\n",
    "    from esm.sdk.api import ESMProtein, GenerationConfig\n",
    "\n",
    "    pdb_files = []\n",
    "    success_count = 0\n",
    "    error_count = 0\n",
    "\n",
    "    # é€æ¡é¢„æµ‹\n",
    "    for rec in tqdm(selected_proteins, desc=\"é¢„æµ‹ç»“æ„\"):\n",
    "        try:\n",
    "            seq = str(rec.seq)\n",
    "            # æ¸…ç† ID ä»¥ç”¨ä½œæ–‡ä»¶å\n",
    "            name = rec.id.replace('|', '_').replace('/', '_').replace('\\\\', '_')[:100]\n",
    "            pdb_file = pipeline.pdb_dir / f\"{name}.pdb\"\n",
    "\n",
    "            # è·³è¿‡å·²å­˜åœ¨çš„æ–‡ä»¶\n",
    "            if pdb_file.exists():\n",
    "                pdb_files.append(pdb_file)\n",
    "                success_count += 1\n",
    "                continue\n",
    "\n",
    "            # ä½¿ç”¨ ESM3 ç”Ÿæˆç»“æ„\n",
    "            protein = ESMProtein(sequence=seq)\n",
    "            protein = pipeline.model.generate(\n",
    "                protein,\n",
    "                GenerationConfig(track='structure', num_steps=NUM_STEPS)\n",
    "            )\n",
    "\n",
    "            # ä¿å­˜ä¸º PDB\n",
    "            protein.to_pdb(str(pdb_file))\n",
    "            pdb_files.append(pdb_file)\n",
    "            success_count += 1\n",
    "\n",
    "        except Exception as e:\n",
    "            print(f\"\\né¢„æµ‹å¤±è´¥ {rec.id}: {e}\")\n",
    "            error_count += 1\n",
    "            continue\n",
    "\n",
    "    print(f\"\\n{'='*60}\")\n",
    "    print(\"ç»“æ„é¢„æµ‹å®Œæˆï¼\")\n",
    "    print(f\"{'='*60}\")\n",
    "    print(f\"æˆåŠŸ: {success_count}\")\n",
    "    print(f\"å¤±è´¥: {error_count}\")\n",
    "    print(f\"PDB æ–‡ä»¶ä¿å­˜åœ¨: {pipeline.pdb_dir}\")\n",
    "\n",
    "except Exception as e:\n",
    "    print(f\"\\n{'='*60}\")\n",
    "    print(\"ESM3 é¢„æµ‹å¤±è´¥\")\n",
    "    print(f\"{'='*60}\")\n",
    "    print(f\"é”™è¯¯: {e}\")\n",
    "    import traceback\n",
    "    traceback.print_exc()\n",
    "\n",
    "    # å³ä½¿å¤±è´¥ï¼Œä¹Ÿæ˜¾ç¤ºå·²å®Œæˆçš„æ•°é‡\n",
    "    completed = list(pipeline.pdb_dir.glob(\"*.pdb\"))\n",
    "    if completed:\n",
    "        print(f\"\\nå·²å®Œæˆ {len(completed)} ä¸ªç»“æ„é¢„æµ‹\")\n",
    "        print(f\"æ–‡ä»¶ä½ç½®: {pipeline.pdb_dir}\")"
   ]
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "## 11. å‡†å¤‡ DALI æ–‡ä»¶å¹¶åˆ›å»ºä¸‹è½½åŒ…\n",
    "\n",
    "å°†é¢„æµ‹çš„ç»“æ„å‡†å¤‡ä¸º DALI æ ¼å¼ï¼Œå¹¶æ‰“åŒ…æ‰€æœ‰ç»“æœ"
   ]
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "try:\n",
    "    # è·å–æ‰€æœ‰å·²ç”Ÿæˆçš„ PDB æ–‡ä»¶\n",
    "    pdb_files = list(pipeline.pdb_dir.glob(\"*.pdb\"))\n",
    "\n",
    "    if len(pdb_files) == 0:\n",
    "        print(\"âš ï¸ æ²¡æœ‰æ‰¾åˆ° PDB æ–‡ä»¶ï¼Œè¯·å…ˆè¿è¡Œ ESM3 é¢„æµ‹\")\n",
    "    else:\n",
    "        print(f\"\\næ‰¾åˆ° {len(pdb_files)} ä¸ª PDB æ–‡ä»¶\")\n",
    "\n",
    "        # å‡†å¤‡ DALI æ–‡ä»¶\n",
    "        pipeline.prepare_for_dali(pdb_files)\n",
    "\n",
    "        # åˆ›å»ºä¸‹è½½åŒ…\n",
    "        result_zip = pipeline.create_download_package(OUTPUT_PREFIX)\n",
    "\n",
    "        print(f\"\\n{'='*60}\")\n",
    "        print(\"å·¥ä½œæµæ‰§è¡ŒæˆåŠŸï¼\")\n",
    "        print(f\"{'='*60}\")\n",
    "        print(f\"\\nç»“æœå·²æ‰“åŒ…åˆ°: {result_zip}\")\n",
    "\n",
    "        # æ˜¾ç¤ºæ–‡ä»¶å¤§å°\n",
    "        size_mb = result_zip.stat().st_size / (1024 * 1024)\n",
    "        print(f\"å‹ç¼©åŒ…å¤§å°: {size_mb:.2f} MB\")\n",
    "\n",
    "except Exception as e:\n",
    "    print(f\"\\n{'='*60}\")\n",
    "    print(\"æ‰“åŒ…å¤±è´¥\")\n",
    "    print(f\"{'='*60}\")\n",
    "    print(f\"é”™è¯¯: {e}\")\n",
    "    import traceback\n",
    "    traceback.print_exc()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 12. ä¸‹è½½ç»“æœ\n",
    "\n",
    "ä¸‹è½½åŒ…å«æ‰€æœ‰ç»“æœçš„å‹ç¼©æ–‡ä»¶"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if IN_COLAB and 'result_zip' in locals():\n",
    "    print(\"æ­£åœ¨å‡†å¤‡ä¸‹è½½...\")\n",
    "    files.download(str(result_zip))\n",
    "    print(\"\\nâœ“ ä¸‹è½½å·²å¼€å§‹ï¼\")\n",
    "    print(\"\\nå‹ç¼©åŒ…å†…å®¹:\")\n",
    "    print(\"  - prokka_output/: Prokka æ³¨é‡Šç»“æœ\")\n",
    "    print(\"  - esm3_structures/: ESM3 é¢„æµ‹çš„ PDB ç»“æ„\")\n",
    "    print(\"  - dali_ready/: ç¬¦åˆ DALI æ ‡å‡†çš„æ–‡ä»¶\")\n",
    "else:\n",
    "    print(f\"ç»“æœæ–‡ä»¶ä½ç½®: {result_zip}\")\n",
    "    print(\"\\nå¦‚æœæ˜¯æœ¬åœ°ç¯å¢ƒï¼Œè¯·ç›´æ¥è®¿é—®ä¸Šè¿°è·¯å¾„è·å–ç»“æœ\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "## 13. æŸ¥çœ‹ç»“æœæ‘˜è¦"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ç»Ÿè®¡ç»“æœ\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"ç»“æœæ‘˜è¦\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "# Prokka ç»“æœ\n",
    "prokka_faa = pipeline.prokka_dir / OUTPUT_PREFIX / f\"{OUTPUT_PREFIX}.faa\"\n",
    "if prokka_faa.exists():\n",
    "    prokka_proteins = list(SeqIO.parse(prokka_faa, \"fasta\"))\n",
    "    print(f\"\\nProkka æ³¨é‡Šç»“æœ:\")\n",
    "    print(f\"  è›‹ç™½è´¨æ•°é‡: {len(prokka_proteins)}\")\n",
    "\n",
    "# ESM3 ç»“æœ\n",
    "pdb_files = list(pipeline.pdb_dir.glob(\"*.pdb\"))\n",
    "print(f\"\\nESM3 ç»“æ„é¢„æµ‹:\")\n",
    "print(f\"  PDB æ–‡ä»¶æ•°: {len(pdb_files)}\")\n",
    "\n",
    "# DALI æ–‡ä»¶\n",
    "dali_files = list(pipeline.dali_dir.glob(\"*.pdb\"))\n",
    "print(f\"\\nDALI è¾“å…¥æ–‡ä»¶:\")\n",
    "print(f\"  å‡†å¤‡å°±ç»ªçš„æ–‡ä»¶: {len(dali_files)}\")\n",
    "\n",
    "print(f\"\\n\" + \"=\"*60)\n",
    "print(\"\\nä¸‹ä¸€æ­¥:\")\n",
    "print(\"1. ä¸‹è½½å¹¶è§£å‹ç»“æœæ–‡ä»¶\")\n",
    "print(\"2. æŸ¥çœ‹ prokka_output/ ä¸­çš„æ³¨é‡Šç»“æœ\")\n",
    "print(\"3. åœ¨ dali_ready/ ä¸­æ‰¾åˆ°å¯ç”¨äº DALI çš„ PDB æ–‡ä»¶\")\n",
    "print(\"4. è®¿é—® DALI æœåŠ¡å™¨è¿›è¡Œç»“æ„æ¯”å¯¹:\")\n",
    "print(\"   http://ekhidna2.biocenter.helsinki.fi/dali/\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "## 14. å¯é€‰ï¼šå•ç‹¬æŸ¥çœ‹æŸä¸ª PDB ç»“æ„"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# åˆ—å‡ºæ‰€æœ‰ç”Ÿæˆçš„ PDB æ–‡ä»¶\n",
    "pdb_files = sorted(pipeline.pdb_dir.glob(\"*.pdb\"))\n",
    "\n",
    "if pdb_files:\n",
    "    print(f\"å…± {len(pdb_files)} ä¸ª PDB æ–‡ä»¶:\\n\")\n",
    "    for i, pdb in enumerate(pdb_files[:10], 1):  # åªæ˜¾ç¤ºå‰10ä¸ª\n",
    "        print(f\"{i}. {pdb.name}\")\n",
    "    \n",
    "    if len(pdb_files) > 10:\n",
    "        print(f\"... è¿˜æœ‰ {len(pdb_files) - 10} ä¸ªæ–‡ä»¶\")\n",
    "    \n",
    "    # å¦‚æœæƒ³æŸ¥çœ‹æŸä¸ªæ–‡ä»¶å†…å®¹ï¼Œå¯ä»¥è¿è¡Œï¼š\n",
    "    # print(\"\\nç¬¬ä¸€ä¸ª PDB æ–‡ä»¶å†…å®¹ï¼ˆå‰20è¡Œï¼‰:\")\n",
    "    # with open(pdb_files[0]) as f:\n",
    "    #     for i, line in enumerate(f):\n",
    "    #         if i >= 20:\n",
    "    #             break\n",
    "    #         print(line.rstrip())\n",
    "else:\n",
    "    print(\"æœªæ‰¾åˆ° PDB æ–‡ä»¶\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## é™„å½•ï¼šæ•…éšœæ’æŸ¥\n",
    "\n",
    "### å¸¸è§é—®é¢˜\n",
    "\n",
    "1. **Prokka å®‰è£…å¤±è´¥**\n",
    "   - ç¡®ä¿æ­£ç¡®å®‰è£…äº† mamba\n",
    "   - å°è¯•é‡æ–°è¿è¡Œå®‰è£…å•å…ƒæ ¼\n",
    "\n",
    "2. **ESM3 æ˜¾å­˜ä¸è¶³**\n",
    "   - å‡å° `MAX_SEQ_LENGTH` å‚æ•°\n",
    "   - ä½¿ç”¨æ›´å°çš„æ¨¡å‹æˆ– CPU æ¨¡å¼\n",
    "\n",
    "3. **Prokka è¿è¡Œæ—¶é—´è¿‡é•¿**\n",
    "   - æ­£å¸¸æƒ…å†µï¼Œå–å†³äºè¾“å…¥æ–‡ä»¶å¤§å°\n",
    "   - å¯ä»¥å¢åŠ  `CPUS` å‚æ•°\n",
    "\n",
    "4. **æ‰¾ä¸åˆ°è›‹ç™½è´¨åºåˆ—**\n",
    "   - æ£€æŸ¥è¾“å…¥çš„ FNA æ–‡ä»¶æ ¼å¼æ˜¯å¦æ­£ç¡®\n",
    "   - ç¡®è®¤ Prokka æˆåŠŸè¿è¡Œ\n",
    "\n",
    "### æ€§èƒ½ä¼˜åŒ–\n",
    "\n",
    "- ä½¿ç”¨ GPU è¿è¡Œæ—¶å¯å¤§å¹…åŠ é€Ÿ ESM3\n",
    "- è°ƒæ•´ `NUM_STEPS` å¹³è¡¡é€Ÿåº¦å’Œè´¨é‡\n",
    "- å¯¹äºå¤§é‡åºåˆ—ï¼Œè€ƒè™‘æ‰¹é‡å¤„ç†"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
